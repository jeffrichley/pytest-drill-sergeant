"""CLI entry point for pytest-drill-sergeant."""

from __future__ import annotations

import json
import logging
import subprocess
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Annotated

import typer
import yaml

from pytest_drill_sergeant.core.analysis_pipeline import create_analysis_pipeline
from pytest_drill_sergeant.core.config_context import (
    get_config,
    get_fail_on_level,
    initialize_config,
    should_fail_on_severity,
)
from pytest_drill_sergeant.core.config_schema import create_default_config
from pytest_drill_sergeant.core.config_validator_enhanced import EnhancedConfigValidator
from pytest_drill_sergeant.core.error_handler import get_error_handler
from pytest_drill_sergeant.core.logging_utils import get_logger, setup_logging
from pytest_drill_sergeant.core.scoring import BRSCalculator, DynamicBISCalculator
from pytest_drill_sergeant.plugin.analysis_storage import (
    AnalysisStorage,
    get_analysis_storage,
)
from pytest_drill_sergeant.plugin.personas.manager import get_persona_manager
from pytest_drill_sergeant.plugin.pytest_cov_integration import PytestCovIntegration

app = typer.Typer(help="pytest-drill-sergeant: AI test quality enforcement")


@dataclass
class LintConfig:
    """Configuration for lint command."""

    paths: list[str]
    profile: str = "standard"
    enable: str | None = None
    disable: str | None = None
    only: str | None = None
    fail_on: str = "error"
    treat: str | None = None
    output_format: str = "terminal"
    output: str | None = None
    config: str | None = None
    persona: str = "drill_sergeant"
    sut_filter: str | None = None
    rich_output: bool = True


class AnalysisContext:
    """Context manager for analysis operations, replacing global state."""

    def __init__(self):
        """Initialize the analysis context."""
        self.storage = AnalysisStorage()
        self.persona_manager = get_persona_manager()
        self.analyzers = []
        self.sut_filter = None

    def add_analyzer(self, analyzer):
        """Add an analyzer to the context."""
        self.storage.add_analyzer(analyzer)
        self.analyzers.append(analyzer)

    def set_sut_filter(self, sut_pattern: str | None):
        """Set the SUT filter pattern."""
        self.sut_filter = SUTFilter(sut_pattern)

    def analyze_file(self, path: Path) -> list:
        """Analyze a single file using the analysis pipeline."""
        # Use the analysis pipeline directly instead of storage
        findings = []
        for analyzer in self.analyzers:
            try:
                analyzer_findings = analyzer.analyze_file(path)
                findings.extend(analyzer_findings)
            except Exception as e:
                # Log error but continue with other analyzers
                logger.error("Error in %s: %s", type(analyzer).__name__, e)
        return findings

    def filter_findings_by_severity(self, findings: list, profile_config=None) -> list:
        """Filter findings based on profile fail-on setting."""
        # Use centralized config context
        try:
            should_fail_func = should_fail_on_severity
        except ImportError:
            # Fallback to passed-in config
            if not profile_config:
                return findings
            should_fail_func = profile_config.should_fail_on_severity

        filtered_findings = []
        for finding in findings:
            # Check if this severity should be included based on fail-on setting
            should_include = should_fail_func(finding.severity)
            if should_include:
                filtered_findings.append(finding)

        # Debug logging
        if findings and len(filtered_findings) != len(findings):
            logger = logging.getLogger(__name__)
            try:
                fail_on = get_fail_on_level().value
            except ImportError:
                fail_on = profile_config.fail_on.value if profile_config else "unknown"
            logger.debug(
                "Filtered %d findings to %d based on fail-on=%s",
                len(findings),
                len(filtered_findings),
                fail_on,
            )

        return filtered_findings

    def get_files_to_analyze(self, base_path: Path) -> list[Path]:
        """Get list of files to analyze based on SUT filter."""
        if self.sut_filter:
            return self.sut_filter.get_analysis_scope(base_path)
        # Default: analyze all Python files
        all_files = list(base_path.rglob("*.py"))
        return [
            f
            for f in all_files
            if not any(
                part.startswith(".") or part == "__pycache__" for part in f.parts
            )
        ]

    def __enter__(self):
        """Enter the analysis context."""
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Exit the analysis context."""
        # Cleanup if needed


class SUTFilter:
    """Filters test files based on SUT (System Under Test) path patterns."""

    def __init__(self, sut_pattern: str | None = None):
        """Initialize the SUT filter.

        Args:
            sut_pattern: Path pattern to filter by (e.g., 'unit', 'unit/utils', 'e2e')
        """
        self.sut_pattern = sut_pattern

    def should_analyze(self, file_path: Path, base_path: Path) -> bool:
        """Check if a file should be analyzed based on SUT filter.

        Args:
            file_path: Path to the file to check
            base_path: Base path for relative comparisons

        Returns:
            True if file should be analyzed, False otherwise
        """
        if not self.sut_pattern:
            return True  # Analyze everything by default

        try:
            # Get relative path from base
            rel_path = file_path.relative_to(base_path)

            # Check if file is under the SUT pattern
            if str(rel_path).startswith(self.sut_pattern):
                return True

            # Also check if any parent directory matches
            return any(str(parent) == self.sut_pattern for parent in rel_path.parents)

        except ValueError:
            # File is not under base_path
            return False

    def get_analysis_scope(self, base_path: Path) -> list[Path]:
        """Get list of files to analyze based on SUT filter.

        Args:
            base_path: Base directory to search from

        Returns:
            List of Python files to analyze
        """
        if not self.sut_pattern:
            # No SUT filter specified - analyze everything in the paths
            all_files = list(base_path.rglob("*.py"))
            return [
                f
                for f in all_files
                if not any(
                    part.startswith(".") or part == "__pycache__" for part in f.parts
                )
            ]

        # SUT filter specified - only analyze files under the SUT pattern
        sut_path = base_path / self.sut_pattern

        if sut_path.exists():
            # SUT path exists, analyze files under it
            return list(sut_path.rglob("*.py"))
        # SUT path doesn't exist, return empty list
        return []


def _run_lint_with_options(opts: LintConfig) -> int:
    """Execute the lint analysis with the given configuration."""
    # Set up logging based on user preference and context
    setup_logging(use_rich=opts.rich_output)
    logger = get_logger(__name__)

    # Initialize error handling
    error_handler = get_error_handler()

    # Initialize centralized configuration

    # Convert CLI options to config dict
    cli_config = {
        "version": "1.0",  # Required field for validation
        "profile": opts.profile,
        "fail_on": opts.fail_on,
        "output_formats": [opts.output_format],
        "verbose": not opts.rich_output,  # Plain output = verbose
        "quiet": False,
    }

    if opts.output:
        cli_config["json_report_path"] = opts.output

    # Handle rule overrides from --treat option
    if opts.treat:
        treat_overrides = {}
        for rule_override in opts.treat.split(","):
            if ":" in rule_override:
                rule_name, severity = rule_override.split(":", 1)
                treat_overrides[rule_name.strip()] = {"level": severity.strip()}
        if treat_overrides:
            cli_config["rules"] = treat_overrides

    # Validate configuration
    config_validator = EnhancedConfigValidator()
    validation_errors = config_validator.validate_config(cli_config)

    if validation_errors:
        typer.echo("‚ùå Configuration validation errors:")
        for error in validation_errors:
            typer.echo(f"  ‚Ä¢ {error.message}")
            if error.suggestion:
                typer.echo(f"    üí° Suggestion: {error.suggestion}")
        return 1

    # Initialize configuration
    try:
        initialize_config(cli_args=cli_config)
        profile_config = get_config()
    except Exception as e:
        logger.error("Failed to load configuration: %s", e)
        typer.echo(f"‚ùå Configuration error: {e}")
        return 1

    logger.info("Starting analysis...")
    logger.info(
        "Analyzing %d paths with %s profile",
        len(opts.paths),
        profile_config.profile.value,
    )
    logger.info("Active profile: %s", profile_config.profile.value)
    logger.info("Fail on: %s", "error")  # DSConfig always fails on error

    # Debug: Show private_access rule configuration
    private_access_rule = profile_config.rules.get("private_access")
    if private_access_rule:
        logger.info(
            "Private access rule severity: %s",
            (
                private_access_rule.severity.value
                if private_access_rule.severity
                else "default"
            ),
        )
        logger.info("Private access rule enabled: %s", private_access_rule.enabled)
    else:
        logger.info("Private access rule: not configured")

    # Debug: Show profile configuration details
    logger.info("Available profiles: %s", list(profile_config.profiles.keys()))
    if "strict" in profile_config.profiles:
        strict_rules = profile_config.profiles["strict"].rules
        logger.info("Strict profile rules: %s", list(strict_rules.keys()))
        if "private_access" in strict_rules:
            logger.info(
                "Strict private_access level: %s",
                (
                    strict_rules["private_access"].severity.value
                    if strict_rules["private_access"].severity
                    else "default"
                ),
            )

    try:
        # Import our analysis components

        # Use context manager instead of globals
        with AnalysisContext() as ctx:
            # Set up SUT filter
            ctx.set_sut_filter(opts.sut_filter)

            # Create centralized analysis pipeline with error handling
            pipeline = create_analysis_pipeline()

            # Add the pipeline to context (context will handle individual analyzers)
            for analyzer in pipeline.analyzers:
                ctx.add_analyzer(analyzer)

            # Analyze each path
            total_violations = 0
            total_files = 0
            all_findings = []
            all_bis_scores = []
            files_analyzed = []

            for path_str in opts.paths:
                path = Path(path_str)

                if path.is_file():
                    # Analyze single file with error handling
                    findings, file_errors = pipeline.analyze_file(path)

                    # Report any analysis errors
                    if file_errors:
                        typer.echo(f"\n‚ö†Ô∏è  Analysis errors in {path}:")
                        for error in file_errors:
                            typer.echo(f"  ‚Ä¢ {error.message}")
                            if error.suggestion:
                                typer.echo(f"    üí° Suggestion: {error.suggestion}")

                    # In advisory mode, show all findings; in strict mode, filter by severity
                    if opts.fail_on == "error":
                        # Advisory mode - show all findings but don't fail on warnings
                        display_findings = findings
                        filtered_findings = ctx.filter_findings_by_severity(findings)
                        total_violations += len(
                            filtered_findings
                        )  # Only count failures for exit code
                    else:
                        # Strict mode - filter and count all
                        display_findings = ctx.filter_findings_by_severity(findings)
                        filtered_findings = display_findings
                        total_violations += len(filtered_findings)

                    total_files += 1

                    # Calculate BIS score for this file
                    bis_calculator = DynamicBISCalculator()
                    metrics = bis_calculator.extract_metrics_from_findings(findings)
                    bis_score = bis_calculator.calculate_bis(metrics)
                    bis_grade = bis_calculator.get_grade(bis_score)

                    # Collect data for BRS calculation
                    all_findings.extend(findings)
                    all_bis_scores.append(bis_score)
                    files_analyzed.append(path)

                    # Display results for this file
                    if display_findings:
                        typer.echo(f"\nüîç Analyzing: {path}")
                        for i, finding in enumerate(display_findings, 1):
                            # Use appropriate message based on severity
                            severity_str = (
                                finding.severity
                                if isinstance(finding.severity, str)
                                else finding.severity.value
                            )
                            if severity_str in ["error"]:
                                message = ctx.persona_manager.on_test_fail(
                                    f"{path.name}:{finding.line_number}", finding
                                )
                                typer.echo(f"  ‚ùå {i}. {message}")
                            else:
                                # For warnings/info, show advisory message with IDE-clickable format
                                typer.echo(
                                    f"  ‚ö†Ô∏è  {i}. {path}:{finding.line_number}: {finding.message}"
                                )

                        # Show BIS score
                        typer.echo(
                            f"  üìä BIS Score: {bis_score:.1f} ({bis_grade}) - {bis_calculator.get_score_interpretation(bis_score)}"
                        )
                    else:
                        success_msg = ctx.persona_manager.on_test_pass(str(path))
                        typer.echo(f"\n‚úÖ {success_msg}")
                        typer.echo(
                            f"  üìä BIS Score: {bis_score:.1f} ({bis_grade}) - {bis_calculator.get_score_interpretation(bis_score)}"
                        )

                elif path.is_dir():
                    # Use SUT filter to get files to analyze
                    files_to_analyze = ctx.get_files_to_analyze(path)

                    if not files_to_analyze:
                        typer.echo(f"‚ö†Ô∏è  No Python files found in {path}")
                        continue

                    # Show filter info if SUT filter is active
                    if opts.sut_filter:
                        typer.echo(
                            f"\nüìÅ Analyzing directory: {path} (SUT filter: {opts.sut_filter})"
                        )
                        typer.echo(
                            f"   Found {len(files_to_analyze)} files matching filter"
                        )
                    else:
                        typer.echo(
                            f"\nüìÅ Analyzing directory: {path} ({len(files_to_analyze)} Python files)"
                        )

                    for py_file in files_to_analyze:
                        findings, file_errors = pipeline.analyze_file(py_file)

                        # Report any analysis errors
                        if file_errors:
                            typer.echo(
                                f"\n  ‚ö†Ô∏è  Analysis errors in {py_file.relative_to(path)}:"
                            )
                            for error in file_errors:
                                typer.echo(f"    ‚Ä¢ {error.message}")
                                if error.suggestion:
                                    typer.echo(
                                        f"      üí° Suggestion: {error.suggestion}"
                                    )

                        # In advisory mode, show all findings; in strict mode, filter by severity
                        if opts.fail_on == "error":
                            # Advisory mode - show all findings but don't fail on warnings
                            display_findings = findings
                            filtered_findings = ctx.filter_findings_by_severity(
                                findings
                            )
                            total_violations += len(
                                filtered_findings
                            )  # Only count failures for exit code
                        else:
                            # Strict mode - filter and count all
                            display_findings = ctx.filter_findings_by_severity(findings)
                            filtered_findings = display_findings
                            total_violations += len(filtered_findings)

                        total_files += 1

                        # Calculate BIS score for this file
                        bis_calculator = DynamicBISCalculator()
                        metrics = bis_calculator.extract_metrics_from_findings(findings)
                        bis_score = bis_calculator.calculate_bis(metrics)
                        bis_grade = bis_calculator.get_grade(bis_score)

                        # Collect data for BRS calculation
                        all_findings.extend(findings)
                        all_bis_scores.append(bis_score)
                        files_analyzed.append(py_file)

                        if display_findings:
                            typer.echo(f"\n  üîç {py_file.relative_to(path)}")
                            for i, finding in enumerate(display_findings, 1):
                                # Use appropriate message based on severity
                                severity_str = (
                                    finding.severity
                                    if isinstance(finding.severity, str)
                                    else finding.severity.value
                                )
                                if severity_str in ["error"]:
                                    message = ctx.persona_manager.on_test_fail(
                                        f"{py_file.name}:{finding.line_number}", finding
                                    )
                                    typer.echo(f"    ‚ùå {i}. {message}")
                                else:
                                    # For warnings/info, show advisory message with IDE-clickable format
                                    typer.echo(
                                        f"    ‚ö†Ô∏è  {i}. {py_file}:{finding.line_number}: {finding.message}"
                                    )

                            # Show BIS score
                            typer.echo(
                                f"    üìä BIS Score: {bis_score:.1f} ({bis_grade}) - {bis_calculator.get_score_interpretation(bis_score)}"
                            )
                        else:
                            # Show BIS score for clean files too
                            typer.echo(
                                f"  ‚úÖ {py_file.relative_to(path)} - BIS Score: {bis_score:.1f} ({bis_grade})"
                            )
                else:
                    typer.echo(f"‚ö†Ô∏è  Path not found: {path}")
                    continue

            # Calculate BRS score
            brs_calculator = BRSCalculator()
            metrics = brs_calculator.extract_metrics_from_analysis(
                files_analyzed, all_findings, all_bis_scores
            )
            brs_score = brs_calculator.calculate_brs(metrics)
            brs_grade = brs_calculator.get_brs_grade(brs_score)
            brs_interpretation = brs_calculator.get_brs_interpretation(brs_score)

            # Display summary
            typer.echo(f"\n{'='*60}")
            summary_msg = ctx.persona_manager.on_summary(
                type(
                    "MockMetrics",
                    (),
                    {
                        "total_tests": total_files,
                        "total_violations": total_violations,
                        "brs_score": brs_score,
                    },
                )()
            )
            typer.echo(f"üéñÔ∏è  {summary_msg}")
            typer.echo(
                f"üìä BRS Score: {brs_score:.1f} ({brs_grade}) - {brs_interpretation}"
            )

            typer.echo(f"üìä Total violations found: {total_violations}")
            typer.echo(f"üìÅ Files analyzed: {total_files}")

            # Display error summary if there were any analysis errors
            analysis_errors = pipeline.get_analysis_errors()
            if analysis_errors:
                typer.echo(f"\n‚ö†Ô∏è  Analysis errors encountered: {len(analysis_errors)}")
                error_summary = pipeline.get_error_summary()
                typer.echo(
                    f"   ‚Ä¢ Critical errors: {error_summary.get('critical_errors', 0)}"
                )
                typer.echo(
                    f"   ‚Ä¢ Recoverable errors: {error_summary.get('recoverable_errors', 0)}"
                )

                # Show error breakdown by category
                by_category = error_summary.get("by_category", {})
                if by_category:
                    typer.echo("   ‚Ä¢ Errors by category:")
                    for category, count in by_category.items():
                        typer.echo(f"     - {category}: {count}")

            return 0 if total_violations == 0 else 1

    except Exception as e:
        logger.error("Analysis failed: %s", e)
        typer.echo(f"‚ùå Analysis failed: {e}")
        return 1


@app.command()
def lint(  # noqa: PLR0913
    paths: Annotated[
        list[str], typer.Argument(..., help="Test files or directories to analyze")
    ],
    profile: Annotated[
        str, typer.Option("--profile", help="Profile to use: chill, standard, strict")
    ] = "standard",
    enable: Annotated[
        str | None,
        typer.Option("--enable", help="Comma-separated list of rules to enable"),
    ] = None,
    disable: Annotated[
        str | None,
        typer.Option("--disable", help="Comma-separated list of rules to disable"),
    ] = None,
    only: Annotated[
        str | None,
        typer.Option(
            "--only", help="Comma-separated list of rules to run (disables all others)"
        ),
    ] = None,
    fail_on: Annotated[
        str,
        typer.Option(
            "--fail-on", help="Severity level to fail on: error, warning, info"
        ),
    ] = "error",
    treat: Annotated[
        str | None,
        typer.Option(
            "--treat",
            help="Override rule severities (e.g., 'duplicate_tests:error,overmocking:info')",
        ),
    ] = None,
    output_format: Annotated[
        str, typer.Option("--format", "-f", help="Output format")
    ] = "terminal",
    output: Annotated[
        str | None, typer.Option("--output", "-o", help="Output file path")
    ] = None,
    config: Annotated[
        str | None, typer.Option("--config", "-c", help="Configuration file")
    ] = None,
    persona: Annotated[
        str, typer.Option("--persona", help="Persona to use")
    ] = "drill_sergeant",
    sut_filter: Annotated[
        str | None,
        typer.Option(
            "--sut-filter", help="SUT path filter (e.g., 'unit', 'unit/utils', 'e2e')"
        ),
    ] = None,
    rich_output: Annotated[
        bool, typer.Option("--rich/--plain", help="Use rich output formatting")
    ] = True,
) -> None:
    """Analyze test files for quality issues."""
    # Create configuration and execute
    opts = LintConfig(
        paths=paths,
        profile=profile,
        enable=enable,
        disable=disable,
        only=only,
        fail_on=fail_on,
        treat=treat,
        output_format=output_format,
        output=output,
        config=config,
        persona=persona,
        sut_filter=sut_filter,
        rich_output=rich_output,
    )
    raise typer.Exit(code=_run_lint_with_options(opts))


@app.command()
def demo(
    persona: str = typer.Option("drill_sergeant", "--persona", help="Persona to use"),
    rich_output: bool = typer.Option(
        True, "--rich/--plain", help="Use rich output formatting"
    ),
) -> None:
    """Run a demo of the plugin with sample tests."""
    # Set up logging based on user preference and context
    setup_logging(use_rich=rich_output)
    logger = get_logger(__name__)

    logger.info("Running demo with %s persona", persona)

    try:
        typer.echo(f"üéñÔ∏è  Running demo with {persona} persona...")
        typer.echo("=" * 60)

        # Initialize systems
        storage = get_analysis_storage()
        persona_manager = get_persona_manager()

        # Add analyzers using centralized pipeline
        # Initialize config context for demo
        initialize_config()  # Use defaults for demo
        pipeline = create_analysis_pipeline()

        # Add all analyzers from pipeline to storage
        for analyzer in pipeline.analyzers:
            storage.add_analyzer(analyzer)

        # Demo with sample files
        sample_files = [
            Path("examples/sample_code/test_with_private_access.py"),
            Path("examples/sample_code/test_clean.py"),
        ]

        total_violations = 0
        total_files = 0

        for sample_file in sample_files:
            if sample_file.exists():
                typer.echo(f"\nüîç Demo: Analyzing {sample_file.name}")
                findings = storage.analyze_test_file(sample_file)
                total_violations += len(findings)
                total_files += 1

                if findings:
                    typer.echo(f"  Found {len(findings)} violations:")
                    for i, finding in enumerate(findings[:3], 1):  # Show first 3
                        message = persona_manager.on_test_fail(
                            f"demo:{finding.line_number}", finding
                        )
                        typer.echo(f"    {i}. {message}")
                    if len(findings) > 3:
                        typer.echo(f"    ... and {len(findings) - 3} more violations")
                else:
                    success_msg = persona_manager.on_test_pass("demo")
                    typer.echo(f"  ‚úÖ {success_msg}")
            else:
                typer.echo(f"‚ö†Ô∏è  Sample file not found: {sample_file}")

        # Demo summary
        typer.echo(f"\n{'='*60}")
        typer.echo("üéñÔ∏è  DEMO SUMMARY")
        brs_score = max(0, 100 - (total_violations * 15))
        summary_msg = persona_manager.on_summary(
            type(
                "MockMetrics",
                (),
                {
                    "total_tests": total_files,
                    "total_violations": total_violations,
                    "brs_score": brs_score,
                },
            )()
        )
        typer.echo(f"üéñÔ∏è  {summary_msg}")
        typer.echo(f"üìä Total violations found: {total_violations}")
        typer.echo(f"üìÅ Files analyzed: {total_files}")

    except Exception as e:
        logger.error("Demo failed: %s", e)
        typer.echo(f"‚ùå Demo failed: {e}")


@app.command()
def profiles(
    rich_output: bool = typer.Option(
        True, "--rich/--plain", help="Use rich output formatting"
    ),
) -> None:
    """List available profiles and their configurations."""
    # Set up logging based on user preference and context
    setup_logging(use_rich=rich_output)
    logger = get_logger(__name__)

    logger.info("Listing available profiles")

    try:
        config = create_default_config()

        typer.echo("üéØ Available Profiles:")
        typer.echo("=" * 50)

        for profile in ["chill", "standard", "strict"]:
            typer.echo(f"\nüìã {profile.upper()} Profile")
            typer.echo("-" * 20)

            if profile == "chill":
                typer.echo("   üßò Low-noise, advisory-only")
                typer.echo("   üìù Most rules are INFO level")
                typer.echo("   üéØ Perfect for legacy repos")
            elif profile == "standard":
                typer.echo("   ‚öñÔ∏è  Balanced approach (default)")
                typer.echo("   ‚ö†Ô∏è  Most rules are WARNING level")
                typer.echo("   üöÄ Good for most projects")
            elif profile == "strict":
                typer.echo("   üî• High standards, CI-ready")
                typer.echo("   ‚ùå Most rules are ERROR level")
                typer.echo("   üéñÔ∏è  Perfect for main branch")

            # Show some example rules for this profile
            typer.echo("\n   Example rules:")
            example_rules = [
                "duplicate_tests",
                "fixture_smells",
                "how_not_what",
                "overmocking",
            ]
            for rule in example_rules:
                if config.is_rule_enabled(rule):
                    severity = config.get_rule_severity(rule)
                    typer.echo(f"     ‚Ä¢ {rule}: {severity.value}")

        typer.echo("\nüí° Usage Examples:")
        typer.echo("   ds lint tests/ --profile standard")
        typer.echo("   ds lint tests/ --profile strict --fail-on warning")
        typer.echo("   ds lint tests/ --enable duplicate_tests,fixture_smells")
        typer.echo("   ds lint tests/ --treat duplicate_tests:error,overmocking:info")

    except Exception as e:
        logger.error("Failed to list profiles: %s", e)
        typer.echo(f"‚ùå Failed to list profiles: {e}")


@app.command()
def personas(
    rich_output: bool = typer.Option(
        True, "--rich/--plain", help="Use rich output formatting"
    ),
) -> None:
    """List available personas."""
    # Set up logging based on user preference and context
    setup_logging(use_rich=rich_output)
    logger = get_logger(__name__)

    logger.info("Listing available personas")

    try:
        persona_manager = get_persona_manager()
        available_personas = persona_manager.list_personas()

        typer.echo("üé≠ Available Personas:")
        typer.echo("=" * 40)

        for persona_name in available_personas:
            try:
                info = persona_manager.get_persona_info(persona_name)
                typer.echo(f"\nüéñÔ∏è  {info['name']}")
                typer.echo(f"   ID: {info['name']}")
                typer.echo(f"   Description: {info['description']}")

                config = info["config"]
                typer.echo(f"   Style: {config.get('communication_style', 'N/A')}")
                typer.echo(f"   Humor: {config.get('humor_level', 'N/A')}")
                typer.echo(f"   Formality: {config.get('formality_level', 'N/A')}")

            except Exception as e:
                typer.echo(f"\n‚ùå Error getting info for {persona_name}: {e}")

        typer.echo("\nüí° Use --persona <name> to select a persona")
        typer.echo("   Example: ds lint tests/ --persona drill_sergeant")

    except Exception as e:
        logger.error("Failed to list personas: %s", e)
        typer.echo(f"‚ùå Failed to list personas: {e}")


def _load_coverage_config(config_file: str) -> dict:
    """Load coverage configuration from file."""
    config_path = Path(config_file)
    if not config_path.exists():
        typer.echo(f"‚ö†Ô∏è  Configuration file not found: {config_file}")
        return {}

    try:
        if config_path.suffix.lower() in [".json"]:
            with open(config_path) as f:
                return json.load(f)
        elif config_path.suffix.lower() in [".yml", ".yaml"]:
            with open(config_path) as f:
                return yaml.safe_load(f)
        else:
            typer.echo(
                f"‚ö†Ô∏è  Unsupported configuration file format: {config_path.suffix}"
            )
            return {}
    except Exception as e:
        typer.echo(f"‚ö†Ô∏è  Failed to load configuration file: {e}")
        return {}


def _process_coverage_analysis(coverage_config: dict, verbose: bool) -> None:
    """Process coverage analysis results."""
    try:
        # Get coverage data from the integration
        integration = PytestCovIntegration()

        # This is a simplified approach - in a real implementation,
        # we'd need to access the coverage data that was collected during pytest execution
        typer.echo("\nüìä Coverage Analysis Results:")

        threshold = coverage_config.get("threshold", 0.0)
        output_file = coverage_config.get("output")
        output_format = coverage_config.get("format", "text")

        if verbose:
            typer.echo(f"  Threshold: {threshold}")
            typer.echo(f"  Output: {output_file or 'console'}")
            typer.echo(f"  Format: {output_format}")

        # Generate coverage analysis report
        report = _generate_coverage_report(coverage_config, verbose)

        # Output the report
        if output_file:
            _write_coverage_report(report, output_file, output_format)
            typer.echo(f"üìÑ Coverage analysis report written to: {output_file}")
        else:
            typer.echo(report)

    except Exception as e:
        typer.echo(f"‚ö†Ô∏è  Failed to process coverage analysis: {e}")


def _generate_coverage_report(coverage_config: dict, verbose: bool) -> str:
    """Generate coverage analysis report."""
    # This is a placeholder implementation
    # In a real implementation, this would analyze the actual coverage data
    # collected during pytest execution

    threshold = coverage_config.get("threshold", 0.0)
    output_format = coverage_config.get("format", "text")

    if output_format == "json":
        report_data = {
            "coverage_analysis": {
                "threshold": threshold,
                "summary": "Coverage analysis completed",
                "timestamp": str(Path.cwd()),
            }
        }
        return json.dumps(report_data, indent=2)
    if output_format == "html":
        return f"""
        <html>
        <head><title>Coverage Analysis Report</title></head>
        <body>
        <h1>Coverage Analysis Report</h1>
        <p>Threshold: {threshold}</p>
        <p>Analysis completed successfully</p>
        </body>
        </html>
        """
    # text format
    return f"""
Coverage Analysis Report
========================
Threshold: {threshold}
Status: Analysis completed successfully
Timestamp: {Path.cwd()}
"""


def _write_coverage_report(report: str, output_file: str, output_format: str) -> None:
    """Write coverage report to file."""
    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, "w") as f:
        f.write(report)


@app.command()
def test(
    paths: Annotated[
        list[str] | None,
        typer.Argument(help="Test files or directories to run"),
    ] = None,
    coverage: bool = typer.Option(
        False, "--coverage", "-c", help="Enable coverage collection with pytest-cov"
    ),
    coverage_source: str = typer.Option(
        "src", "--cov-source", help="Source directory for coverage collection"
    ),
    coverage_fail_under: int = typer.Option(
        0, "--cov-fail-under", help="Minimum coverage percentage to pass"
    ),
    ds_coverage_threshold: float = typer.Option(
        0.0,
        "--ds-coverage-threshold",
        help="Minimum CAR score threshold for coverage analysis",
    ),
    ds_coverage_output: str = typer.Option(
        None, "--ds-coverage-output", help="Output file for coverage analysis results"
    ),
    ds_coverage_format: str = typer.Option(
        "text",
        "--ds-coverage-format",
        help="Output format for coverage analysis (text, json, html)",
    ),
    config_file: str = typer.Option(
        None, "--config", help="Configuration file for coverage settings"
    ),
    verbose: bool = typer.Option(False, "--verbose", "-v", help="Verbose output"),
    rich_output: bool = typer.Option(
        True, "--rich/--plain", help="Use rich output formatting"
    ),
) -> None:
    """Run tests with pytest and optional coverage analysis."""
    # Set up logging based on user preference and context
    setup_logging(use_rich=rich_output)
    logger = get_logger(__name__)

    logger.info("Running tests with pytest-drill-sergeant integration")

    try:
        # Load configuration file if provided
        coverage_config = {}
        if config_file:
            coverage_config = _load_coverage_config(config_file)
            # Override with command line options
            if ds_coverage_threshold != 0.0:
                coverage_config["threshold"] = ds_coverage_threshold
            if ds_coverage_output:
                coverage_config["output"] = ds_coverage_output
            if ds_coverage_format != "text":
                coverage_config["format"] = ds_coverage_format
        else:
            # Use command line options directly
            coverage_config = {
                "threshold": ds_coverage_threshold,
                "output": ds_coverage_output,
                "format": ds_coverage_format,
            }

        # Set default paths if none provided
        if not paths:
            paths = ["tests/"]

        # Build pytest command
        cmd = [sys.executable, "-m", "pytest"]

        # Add paths
        cmd.extend(paths)

        # Add verbose flag if requested
        if verbose:
            cmd.append("-v")

        # Add coverage options if requested
        if coverage:
            cmd.extend(
                [
                    f"--cov={coverage_source}",
                    f"--cov-fail-under={coverage_fail_under}",
                ]
            )
            typer.echo("üéØ Running tests with coverage analysis...")
        else:
            typer.echo("üß™ Running tests...")

        typer.echo(f"Command: {' '.join(cmd)}")
        typer.echo("=" * 60)

        # Run pytest
        result = subprocess.run(cmd, cwd=Path.cwd(), check=False)

        # Process coverage analysis if coverage was enabled
        if coverage and result.returncode == 0:
            _process_coverage_analysis(coverage_config, verbose)

        if result.returncode == 0:
            typer.echo("\n‚úÖ All tests passed!")
        else:
            typer.echo(f"\n‚ùå Tests failed with exit code {result.returncode}")
            sys.exit(result.returncode)

    except Exception as e:
        logger.error("Failed to run tests: %s", e)
        typer.echo(f"‚ùå Failed to run tests: {e}")
        sys.exit(1)


def cli() -> None:
    """CLI entry point."""
    app()


if __name__ == "__main__":
    cli()
